# ğŸ§  DICOM Specialist

A lightweight but powerful [fastMCP](https://github.com/grobian/fastmcp) servers for navigating the complexity of the DICOM standard.

## Features:
- ğŸ” **RAG (Retrieval-Augmented Generation)** over the DICOM standard documentation.
- ğŸ“š **Knowledge Tools**:
  - List and explore DICOM *CIUDs*, *modules*, and *attributes*
  - Look up detailed info on specific attributes
  - Find and explain UIDs quickly

## ğŸ§ª Tech Stack

These servers are built with performance and local inference in mind:

| Component | Description |
|----------|-------------|
| ğŸ§  **LLM Engine** | [`Jan Nano 4B`](https://github.com/janhq/jan) â€“ a lightweight, local LLM |
| âš™ï¸ **Server Framework** | [`fastMCP`](https://github.com/grobian/fastmcp) â€“ fast, minimal LLM HTTP API |
| ğŸ“¦ **Vector DB** | [DuckDB](https://duckdb.org) â€“ efficient, in-process database |
| ğŸ§© **Embeddings** | `all-MiniLM-L6-v2` via [`llama-index`](https://github.com/jerryjliu/llama_index) |
| ğŸ§  **Chunking** | Optional AI-assisted chunk context generation for better retrieval quality |


## ğŸš€ Getting Started

The project is managed using *uv*. You can start the server using `uv run dicom-knowledge-mcp`. To run in Jan.ai run `uv build`, followed by `uv tools install`. Afterwards it can be used with `uvx dicom-knowledge-mcp`.

## ğŸ™Œ Acknowledgements

- [Jan LLM](https://github.com/janhq/jan)
- [fastMCP](https://github.com/grobian/fastmcp)
- [LlamaIndex](https://github.com/jerryjliu/llama_index)
- [DuckDB](https://duckdb.org)
- [MiniLM](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
- [SentenceTransformer ğŸ¤—](https://huggingface.co/sentence-transformers)
